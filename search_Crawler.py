import os
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ê²€ìƒ‰ì–´ ì…ë ¥
keyword = input("ğŸ” í¬ë¡¤ë§í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

# JSON ë¡œë“œ
with open("gknu_results.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# í‚¤ì›Œë“œ í•„í„°ë§
filtered_data = [item for item in data if keyword in item["title"]]

if not filtered_data:
    print("âŒ í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# ì €ì¥ í´ë” ì¤€ë¹„
os.makedirs("download", exist_ok=True)
os.makedirs("contents", exist_ok=True)

base_url = "https://www.gknu.ac.kr"

# í•„í„°ë§ëœ í•­ëª© ì²˜ë¦¬
for idx, item in enumerate(filtered_data, start=1):
    title = item["title"]
    url = item["url"]
    print(f"\nğŸ” [{idx}] ì²˜ë¦¬ ì¤‘: {title}")

    # í˜ì´ì§€ ìš”ì²­
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ
    content_div = soup.select_one("div.board-view-cont")
    if content_div:
        spans = content_div.select("p span")
        full_text = "\n".join(span.get_text(strip=True) for span in spans if span.get_text(strip=True))

        # ì €ì¥ íŒŒì¼ ì´ë¦„ì— ê²€ìƒ‰ì–´ í¬í•¨
        text_file = f"contents/{keyword}_Page{idx}.txt"
        with open(text_file, "w", encoding="utf-8") as f:
            f.write(full_text)
        print(f"ğŸ“ ë³¸ë¬¸ ì €ì¥ ì™„ë£Œ: {text_file}")

        # ì´ë¯¸ì§€ URL ì¶œë ¥
        images = [urljoin(base_url, img["src"]) for img in content_div.find_all("img") if img.get("src")]
        for img_url in images:
            print(f"ğŸ“¸ ì´ë¯¸ì§€ URL: {img_url}")
    else:
        print("âŒ ë³¸ë¬¸ ì½˜í…ì¸  ì—†ìŒ")

    # âœ… ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
    clip_div = soup.select_one("div.clip")
    if clip_div:
        for i, a_tag in enumerate(clip_div.find_all("a", href=True), start=1):
            file_url = urljoin(base_url, a_tag["href"])
            file_ext = os.path.splitext(file_url)[1]
            file_name = f"{keyword}_Page{idx}_File{i}{file_ext}"

            try:
                file_data = requests.get(file_url)
                file_path = os.path.join("download", file_name)

                with open(file_path, "wb") as f:
                    f.write(file_data.content)
                print(f"âœ… ì²¨ë¶€íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            except Exception as e:
                print(f"âŒ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_url} / ì´ìœ : {e}")
    else:
        print("ğŸ“‚ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
